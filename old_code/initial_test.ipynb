{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crt_question</th>\n",
       "      <th>conf_proba</th>\n",
       "      <th>conf_subjective</th>\n",
       "      <th>correct</th>\n",
       "      <th>intuitive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A scarf costs 210€ more than a hat. The scarf ...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How long would it take 80 carpenters to repair...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>8</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An entire forest was consumed by a wildfire in...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If Andrea can clean a house in 3 hours, and Al...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A runner participates in a marathon and arrive...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>199</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A woman buys a second-hand car for $1000, then...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Frank decided to invest $10,000 into bitcoin i...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>9000</td>\n",
       "      <td>18000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        crt_question  \\\n",
       "0  A scarf costs 210€ more than a hat. The scarf ...   \n",
       "1  How long would it take 80 carpenters to repair...   \n",
       "2  An entire forest was consumed by a wildfire in...   \n",
       "3  If Andrea can clean a house in 3 hours, and Al...   \n",
       "4  A runner participates in a marathon and arrive...   \n",
       "5  A woman buys a second-hand car for $1000, then...   \n",
       "6  Frank decided to invest $10,000 into bitcoin i...   \n",
       "\n",
       "                                          conf_proba  \\\n",
       "0  Give the probability with which you answered c...   \n",
       "1  Give the probability with which you answered c...   \n",
       "2  Give the probability with which you answered c...   \n",
       "3  Give the probability with which you answered c...   \n",
       "4  Give the probability with which you answered c...   \n",
       "5  Give the probability with which you answered c...   \n",
       "6  Give the probability with which you answered c...   \n",
       "\n",
       "                                     conf_subjective  correct  intuitive  \n",
       "0  How sure are you in your response? Choose betw...        5         10  \n",
       "1  How sure are you in your response? Choose betw...        8         80  \n",
       "2  How sure are you in your response? Choose betw...       39         20  \n",
       "3  How sure are you in your response? Choose betw...        2          9  \n",
       "4  How sure are you in your response? Choose betw...      199        200  \n",
       "5  How sure are you in your response? Choose betw...     2000       1000  \n",
       "6  How sure are you in your response? Choose betw...     9000      18000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the data path relative to the current working directory\n",
    "data_path = os.path.join('.', 'data', 'crt_confidence.csv')\n",
    "\n",
    "# Load the crt_confidence.csv file\n",
    "crt_data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "crt_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ndif-team/nnsight.git@0.3\n",
      "  Cloning https://github.com/ndif-team/nnsight.git (to revision 0.3) to /private/var/folders/gv/001yhxz1531ct7td9p0zgrh80000gn/T/pip-req-build-y_eovcgb\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ndif-team/nnsight.git /private/var/folders/gv/001yhxz1531ct7td9p0zgrh80000gn/T/pip-req-build-y_eovcgb\n",
      "  Running command git checkout -b 0.3 --track origin/0.3\n",
      "  Switched to a new branch '0.3'\n",
      "  branch '0.3' set up to track 'origin/0.3'.\n",
      "  Resolved https://github.com/ndif-team/nnsight.git to commit ca7c918f4e3672c0771ba247149cd8e254eeefb3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from nnsight==0.2.22.dev215+gca7c918) (4.45.0.dev0)\n",
      "Requirement already satisfied: protobuf in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from nnsight==0.2.22.dev215+gca7c918) (5.27.3)\n",
      "Requirement already satisfied: python-socketio[client] in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from nnsight==0.2.22.dev215+gca7c918) (5.11.3)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from nnsight==0.2.22.dev215+gca7c918) (0.19.1)\n",
      "Requirement already satisfied: pydantic>=2.4.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from nnsight==0.2.22.dev215+gca7c918) (2.8.2)\n",
      "Requirement already satisfied: torch>=2.1.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from nnsight==0.2.22.dev215+gca7c918) (2.4.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from nnsight==0.2.22.dev215+gca7c918) (0.2.0)\n",
      "Requirement already satisfied: torchvision in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from nnsight==0.2.22.dev215+gca7c918) (0.19.0)\n",
      "Requirement already satisfied: accelerate in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from nnsight==0.2.22.dev215+gca7c918) (0.33.0)\n",
      "Requirement already satisfied: diffusers in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from nnsight==0.2.22.dev215+gca7c918) (0.30.0)\n",
      "Requirement already satisfied: einops in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from nnsight==0.2.22.dev215+gca7c918) (0.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from pydantic>=2.4.0->nnsight==0.2.22.dev215+gca7c918) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from pydantic>=2.4.0->nnsight==0.2.22.dev215+gca7c918) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from pydantic>=2.4.0->nnsight==0.2.22.dev215+gca7c918) (4.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from tokenizers>=0.13.0->nnsight==0.2.22.dev215+gca7c918) (0.24.5)\n",
      "Requirement already satisfied: filelock in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from torch>=2.1.0->nnsight==0.2.22.dev215+gca7c918) (3.15.4)\n",
      "Requirement already satisfied: sympy in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from torch>=2.1.0->nnsight==0.2.22.dev215+gca7c918) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from torch>=2.1.0->nnsight==0.2.22.dev215+gca7c918) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from torch>=2.1.0->nnsight==0.2.22.dev215+gca7c918) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from torch>=2.1.0->nnsight==0.2.22.dev215+gca7c918) (2024.6.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from accelerate->nnsight==0.2.22.dev215+gca7c918) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from accelerate->nnsight==0.2.22.dev215+gca7c918) (24.1)\n",
      "Requirement already satisfied: psutil in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from accelerate->nnsight==0.2.22.dev215+gca7c918) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from accelerate->nnsight==0.2.22.dev215+gca7c918) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from accelerate->nnsight==0.2.22.dev215+gca7c918) (0.4.4)\n",
      "Requirement already satisfied: importlib-metadata in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from diffusers->nnsight==0.2.22.dev215+gca7c918) (8.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from diffusers->nnsight==0.2.22.dev215+gca7c918) (2024.7.24)\n",
      "Requirement already satisfied: requests in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from diffusers->nnsight==0.2.22.dev215+gca7c918) (2.32.3)\n",
      "Requirement already satisfied: Pillow in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from diffusers->nnsight==0.2.22.dev215+gca7c918) (10.4.0)\n",
      "Requirement already satisfied: bidict>=0.21.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from python-socketio[client]->nnsight==0.2.22.dev215+gca7c918) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.8.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from python-socketio[client]->nnsight==0.2.22.dev215+gca7c918) (4.9.1)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from python-socketio[client]->nnsight==0.2.22.dev215+gca7c918) (1.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from transformers->nnsight==0.2.22.dev215+gca7c918) (4.66.5)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from python-engineio>=4.8.0->python-socketio[client]->nnsight==0.2.22.dev215+gca7c918) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from requests->diffusers->nnsight==0.2.22.dev215+gca7c918) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from requests->diffusers->nnsight==0.2.22.dev215+gca7c918) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from requests->diffusers->nnsight==0.2.22.dev215+gca7c918) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from requests->diffusers->nnsight==0.2.22.dev215+gca7c918) (2024.7.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from importlib-metadata->diffusers->nnsight==0.2.22.dev215+gca7c918) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from jinja2->torch>=2.1.0->nnsight==0.2.22.dev215+gca7c918) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from sympy->torch>=2.1.0->nnsight==0.2.22.dev215+gca7c918) (1.3.0)\n",
      "Requirement already satisfied: wsproto in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio[client]->nnsight==0.2.22.dev215+gca7c918) (1.2.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages (from wsproto->simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio[client]->nnsight==0.2.22.dev215+gca7c918) (0.14.0)\n",
      "Building wheels for collected packages: nnsight\n",
      "  Building wheel for nnsight (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nnsight: filename=nnsight-0.2.22.dev215+gca7c918-py3-none-any.whl size=3529064 sha256=2cbe1a9530a49c015b5961332adccacdb96228eb235ae504ea70a6a91029c379\n",
      "  Stored in directory: /private/var/folders/gv/001yhxz1531ct7td9p0zgrh80000gn/T/pip-ephem-wheel-cache-g_x85bd5/wheels/fe/c8/ae/03b2b3d4ab9b88a2cc05965ec73a690db626fd3d604a533a6d\n",
      "Successfully built nnsight\n",
      "Installing collected packages: nnsight\n",
      "  Attempting uninstall: nnsight\n",
      "    Found existing installation: nnsight 0.2.21\n",
      "    Uninstalling nnsight-0.2.21:\n",
      "      Successfully uninstalled nnsight-0.2.21\n",
      "Successfully installed nnsight-0.2.22.dev215+gca7c918\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ndif-team/nnsight.git@0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import CONFIG\n",
    "\n",
    "CONFIG.set_default_api_key(\"546253536da74e51a1ccdfe887eafcea\")\n",
    "os.environ['HF_TOKEN'] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 8192)\n",
      "    (layers): ModuleList(\n",
      "      (0-79): 80 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "          (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "          (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "          (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((8192,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=128256, bias=False)\n",
      "  (generator): WrapperModule()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\"meta-llama/Meta-Llama-3.1-70B-Instruct\", device_map=\"auto\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaRotaryEmbedding()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 08:28:39,794 MainProcess nnsight_remote INFO     66c0c1a703d921d8b8d8a164 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2024-08-17 08:28:39,862 MainProcess nnsight_remote INFO     66c0c1a703d921d8b8d8a164 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2024-08-17 08:28:40,916 MainProcess nnsight_remote INFO     66c0c1a703d921d8b8d8a164 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.50k/1.50k [00:00<00:00, 12.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[21000,    13,   374,   400,   605,    15,    13,  1109,   264,  9072,\n",
      "            13,  1442,  2860,  7194,   279,  9072,  3871,   220, 12819, 15406,\n",
      "          3871,  2860,    13,  2650,  1790,  1587,   279,  9072,  2853,  5380,\n",
      "           865,    25,   264,  1396,  1193,    11,   220]])\n",
      "Prediction:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with model.trace(crt_data['crt_question'][0], remote=True):\n",
    "\n",
    "    # Access the last layer using h[-1] as it's a ModuleList\n",
    "    # Access the first index of .output as that's where the hidden states are.\n",
    "    model.model.layers[-1].mlp.output[0][:] = 0\n",
    "\n",
    "    # Logits come out of model.lm_head and we apply argmax to get the predicted token ids.\n",
    "    token_ids = model.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "print(\"Token IDs:\", token_ids)\n",
    "# Apply the tokenizer to decode the ids into words after the tracing context.\n",
    "print(\"Prediction:\", model.tokenizer.decode(token_ids[0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaModel' object has no attribute 'lm_head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrt_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcrt_question\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#token_ids_2 = model.model.lm_head.next().output.argmax(dim=-1).save()\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#token_ids_3 = model.model.lm_head.next().output.argmax(dim=-1).save()\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages/nnsight/contexts/Tracer.py:83\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39malive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoker_inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo input was provided to the tracing context.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgenerate(crt_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrt_question\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     token_ids_1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#token_ids_2 = model.model.lm_head.next().output.argmax(dim=-1).save()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#token_ids_3 = model.model.lm_head.next().output.argmax(dim=-1).save()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages/nnsight/envoy.py:400\u001b[0m, in \u001b[0;36mEnvoy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Envoy, Any]:\n\u001b[1;32m    391\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper method for underlying module's attributes.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m        Any: Attribute.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module, key)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaModel' object has no attribute 'lm_head'"
     ]
    }
   ],
   "source": [
    "with model.generate(crt_data['crt_question'][0], max_new_tokens=3):\n",
    "\n",
    "    token_ids_1 = model.model.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "    #token_ids_2 = model.model.lm_head.next().output.argmax(dim=-1).save()\n",
    "\n",
    "    #token_ids_3 = model.model.lm_head.next().output.argmax(dim=-1).save()\n",
    "\n",
    "    output = model.generator.output.save()\n",
    "\n",
    "print(\"Prediction 1: \", model.tokenizer.decode(token_ids_1[0][-1]))\n",
    "print(\"Prediction 2: \", model.tokenizer.decode(token_ids_2[0][-1]))\n",
    "print(\"Prediction 3: \", model.tokenizer.decode(token_ids_3[0][-1]))\n",
    "\n",
    "print(\"All token ids: \", output)\n",
    "\n",
    "print(\"All prediction: \", model.tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all crt_question prompts\n",
    "all_prompts = crt_data['crt_question'].tolist()\n",
    "\n",
    "# get all conf_proba prompts\n",
    "all_conf_proba = crt_data['conf_proba'].tolist()\n",
    "\n",
    "# get all conf_subjective prompts\n",
    "all_conf_subjective = crt_data['conf_subjective'].tolist()\n",
    "\n",
    "# get correct answers\n",
    "correct_answers = crt_data['correct']\n",
    "\n",
    "# get intuitive answers\n",
    "intuitive_answers = crt_data['intuitive']\n",
    "\n",
    "# tokenize all prompts\n",
    "prompt_tokens = model.tokenizer(all_prompts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   13,    13, 23998,    13,  1374,    13,   198], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "with model.trace() as tracer:\n",
    "\n",
    "    with tracer.invoke(prompt_tokens) as invoker:\n",
    "\n",
    "        output = model.lm_head.output.argmax(dim=-1).save()\n",
    "        \n",
    "\n",
    "print(output[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 65])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [22:09<00:00, 443.27s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:32<00:00, 10.87s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/Users/adamimos/anaconda3/envs/LLM-confidence/lib/python3.11/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate English to French: How are you?\n",
      "\n",
      "In French, the question \"How\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "#tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "#model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define your instruction\n",
    "instruction = \"Translate English to French: How are you?\"\n",
    "\n",
    "# Tokenize and generate output\n",
    "input_ids = tokenizer(instruction, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "# Decode and print the result\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Row 1\n",
      "================================================================================\n",
      "Instruction:         A scarf costs 210€ more than a hat. The scarf and the hat cost 220€ in total. How much does the hat cost? Answer with one number only.\n",
      "Answer:              The scarf cost 220€ x 220 = $120. The scarf cost 220        \n",
      "Conf. Proba Prompt:  Give the probability with which you answered correctly between 0 and 1.\n",
      "Conf. Proba Answer:  0                                                           \n",
      "Conf. Subj Prompt:   How sure are you in your response? Choose between (very sure, sure, somewhat unsure, unsure)\n",
      "Conf. Subj Answer:   Answer: very sure, sure, somewhat unsure, unsure            \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Row 2\n",
      "================================================================================\n",
      "Instruction:         How long would it take 80 carpenters to repair 80 tables, if it takes 8 carpenters 8 hours to repair 8 tables?  Answer with one number only.\n",
      "Answer:              8 hours                                                     \n",
      "Conf. Proba Prompt:  Give the probability with which you answered correctly between 0 and 1.\n",
      "Conf. Proba Answer:  1                                                           \n",
      "Conf. Subj Prompt:   How sure are you in your response? Choose between (very sure, sure, somewhat unsure, unsure)\n",
      "Conf. Subj Answer:   Answer: very sure                                           \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Row 3\n",
      "================================================================================\n",
      "Instruction:         An entire forest was consumed by a wildfire in 40 hours, with its size doubling every hour. How long did it take to burn 50% of the forest? Answer with one number only.\n",
      "Answer:              40 hours                                                    \n",
      "Conf. Proba Prompt:  Give the probability with which you answered correctly between 0 and 1.\n",
      "Conf. Proba Answer:  0                                                           \n",
      "Conf. Subj Prompt:   How sure are you in your response? Choose between (very sure, sure, somewhat unsure, unsure)\n",
      "Conf. Subj Answer:   Answer: very sure                                           \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Row 4\n",
      "================================================================================\n",
      "Instruction:         If Andrea can clean a house in 3 hours, and Alex can clean a house in 6 hours, how many hours would it take for them to clean a house together? Answer with one number only.\n",
      "Answer:              3                                                           \n",
      "Conf. Proba Prompt:  Give the probability with which you answered correctly between 0 and 1.\n",
      "Conf. Proba Answer:  0                                                           \n",
      "Conf. Subj Prompt:   How sure are you in your response? Choose between (very sure, sure, somewhat unsure, unsure)\n",
      "Conf. Subj Answer:   3                                                           \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Row 5\n",
      "================================================================================\n",
      "Instruction:         A runner participates in a marathon and arrives both at the 100th highest and the 100th lowest position. How many participants are in the marathon? Answer with one number only.\n",
      "Answer:              1                                                           \n",
      "Conf. Proba Prompt:  Give the probability with which you answered correctly between 0 and 1.\n",
      "Conf. Proba Answer:  0                                                           \n",
      "Conf. Subj Prompt:   How sure are you in your response? Choose between (very sure, sure, somewhat unsure, unsure)\n",
      "Conf. Subj Answer:   (very sure, sure, somewhat unsure, unsure)                  \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Row 6\n",
      "================================================================================\n",
      "Instruction:         A woman buys a second-hand car for $1000, then sells it for $2000. Later she buys it back for $3000 and finally sells it for $4000. How much has she made? Answer with one number only.\n",
      "Answer:              $1000 - $2000 = $1000. She has made $1000 + $4000 =         \n",
      "Conf. Proba Prompt:  Give the probability with which you answered correctly between 0 and 1.\n",
      "Conf. Proba Answer:  0                                                           \n",
      "Conf. Subj Prompt:   How sure are you in your response? Choose between (very sure, sure, somewhat unsure, unsure)\n",
      "Conf. Subj Answer:   Answer: very sure                                           \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Row 7\n",
      "================================================================================\n",
      "Instruction:         Frank decided to invest $10,000 into bitcoin in January 2018. Four months after he invested, the bitcoin he had purchased went down 50%. In the subsequent eight months, the bitcoin he had purchased went up 80%. What is the value of Frank’s bitcoin after one year?  Answer with one number only.\n",
      "Answer:              10                                                          \n",
      "Conf. Proba Prompt:  Give the probability with which you answered correctly between 0 and 1.\n",
      "Conf. Proba Answer:  0                                                           \n",
      "Conf. Subj Prompt:   How sure are you in your response? Choose between (very sure, sure, somewhat unsure, unsure)\n",
      "Conf. Subj Answer:   (very sure, sure, somewhat unsure, unsure)                  \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Function to process a single row\n",
    "def process_row(row):\n",
    "    # Define your instruction\n",
    "    instruction = row['crt_question']\n",
    "\n",
    "    # Tokenize and generate output\n",
    "    input_ids = tokenizer(instruction, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids)\n",
    "\n",
    "    # Decode and print the result\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Combine instruction and result for confidence prompts\n",
    "    combined_prompt = f\"{instruction}\\nAnswer: {result}\"\n",
    "\n",
    "    # Confidence probability prompt\n",
    "    conf_proba_prompt = combined_prompt + \"\\n\" + row['conf_proba']\n",
    "    conf_proba_input = tokenizer(conf_proba_prompt, return_tensors=\"pt\").input_ids\n",
    "    conf_proba_output = model.generate(conf_proba_input)\n",
    "    conf_proba_result = tokenizer.decode(conf_proba_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Confidence subjective prompt\n",
    "    conf_subj_prompt = combined_prompt + \"\\n\" + row['conf_subjective']\n",
    "    conf_subj_input = tokenizer(conf_subj_prompt, return_tensors=\"pt\").input_ids\n",
    "    conf_subj_output = model.generate(conf_subj_input)\n",
    "    conf_subj_result = tokenizer.decode(conf_subj_output[0], skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"Instruction\": instruction,\n",
    "        \"Answer\": result,\n",
    "        \"Conf. Proba Prompt\": row['conf_proba'],\n",
    "        \"Conf. Proba Answer\": conf_proba_result,\n",
    "        \"Conf. Subj Prompt\": row['conf_subjective'],\n",
    "        \"Conf. Subj Answer\": conf_subj_result\n",
    "    }\n",
    "\n",
    "# Process all rows\n",
    "results = []\n",
    "for _, row in crt_data.iterrows():\n",
    "    results.append(process_row(row))\n",
    "\n",
    "# Print results in a formatted table\n",
    "for i, result in enumerate(results):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Row {i + 1}\")\n",
    "    print(\"=\" * 80)\n",
    "    for key, value in result.items():\n",
    "        print(\"{:<20} {:<60}\".format(key + \":\", value))\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A scarf costs 210€ more than a hat. The scarf and the hat cost 220€ in total. How much does the hat cost? Answer with one number only.\\nAnswer: The scarf cost 220€ x 220 = $120. The scarf cost 220\\nGive the probability with which you answered correctly between 0 and 1.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_proba_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: A scarf costs 210€ more than a hat. The scarf and the hat cost 220€ in total. How much does the hat cost? Answer with one number only.\n",
      "Response: The scarf cost 220€ x 220 = $120. The scarf cost 220\n"
     ]
    }
   ],
   "source": [
    "# print instruction and response in a very pretty table that i can copy paste into a paper\n",
    "print(f\"Instruction: {instruction}\")\n",
    "print(f\"Response: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crt_question</th>\n",
       "      <th>conf_proba</th>\n",
       "      <th>conf_subjective</th>\n",
       "      <th>correct</th>\n",
       "      <th>intuitive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A scarf costs 210€ more than a hat. The scarf ...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How long would it take 80 carpenters to repair...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>8</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An entire forest was consumed by a wildfire in...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If Andrea can clean a house in 3 hours, and Al...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A runner participates in a marathon and arrive...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>199</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A woman buys a second-hand car for $1000, then...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Frank decided to invest $10,000 into bitcoin i...</td>\n",
       "      <td>Give the probability with which you answered c...</td>\n",
       "      <td>How sure are you in your response? Choose betw...</td>\n",
       "      <td>9000</td>\n",
       "      <td>18000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        crt_question  \\\n",
       "0  A scarf costs 210€ more than a hat. The scarf ...   \n",
       "1  How long would it take 80 carpenters to repair...   \n",
       "2  An entire forest was consumed by a wildfire in...   \n",
       "3  If Andrea can clean a house in 3 hours, and Al...   \n",
       "4  A runner participates in a marathon and arrive...   \n",
       "5  A woman buys a second-hand car for $1000, then...   \n",
       "6  Frank decided to invest $10,000 into bitcoin i...   \n",
       "\n",
       "                                          conf_proba  \\\n",
       "0  Give the probability with which you answered c...   \n",
       "1  Give the probability with which you answered c...   \n",
       "2  Give the probability with which you answered c...   \n",
       "3  Give the probability with which you answered c...   \n",
       "4  Give the probability with which you answered c...   \n",
       "5  Give the probability with which you answered c...   \n",
       "6  Give the probability with which you answered c...   \n",
       "\n",
       "                                     conf_subjective  correct  intuitive  \n",
       "0  How sure are you in your response? Choose betw...        5         10  \n",
       "1  How sure are you in your response? Choose betw...        8         80  \n",
       "2  How sure are you in your response? Choose betw...       39         20  \n",
       "3  How sure are you in your response? Choose betw...        2          9  \n",
       "4  How sure are you in your response? Choose betw...      199        200  \n",
       "5  How sure are you in your response? Choose betw...     2000       1000  \n",
       "6  How sure are you in your response? Choose betw...     9000      18000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-confidence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
